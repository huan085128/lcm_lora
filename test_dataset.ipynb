{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import json\n",
    "import math\n",
    "from typing import List, Union\n",
    "import torchvision.transforms.functional as TF\n",
    "import webdataset as wds\n",
    "from braceexpand import braceexpand\n",
    "from torch.utils.data import default_collate\n",
    "from torchvision import transforms\n",
    "from webdataset.tariterators import (\n",
    "    base_plus_ext,\n",
    "    tar_file_expander,\n",
    "    url_opener,\n",
    "    valid_sample,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tarfile_to_samples_nothrow(src, handler=wds.warn_and_continue):\n",
    "    # NOTE this is a re-impl of the webdataset impl with group_by_keys that doesn't throw\n",
    "    streams = url_opener(src, handler=handler)\n",
    "    files = tar_file_expander(streams, handler=handler)\n",
    "    samples = group_by_keys_nothrow(files, handler=handler)\n",
    "    return samples\n",
    "\n",
    "\n",
    "def group_by_keys_nothrow(data, keys=base_plus_ext, lcase=True, suffixes=None, handler=None):\n",
    "    \"\"\"Return function over iterator that groups key, value pairs into samples.\n",
    "\n",
    "    :param keys: function that splits the key into key and extension (base_plus_ext) :param lcase: convert suffixes to\n",
    "    lower case (Default value = True)\n",
    "    \"\"\"\n",
    "    current_sample = None\n",
    "    for filesample in data:\n",
    "        assert isinstance(filesample, dict)\n",
    "        fname, value = filesample[\"fname\"], filesample[\"data\"]\n",
    "        prefix, suffix = keys(fname)\n",
    "        if prefix is None:\n",
    "            continue\n",
    "        if lcase:\n",
    "            suffix = suffix.lower()\n",
    "        # FIXME webdataset version throws if suffix in current_sample, but we have a potential for\n",
    "        #  this happening in the current LAION400m dataset if a tar ends with same prefix as the next\n",
    "        #  begins, rare, but can happen since prefix aren't unique across tar files in that dataset\n",
    "        if current_sample is None or prefix != current_sample[\"__key__\"] or suffix in current_sample:\n",
    "            if valid_sample(current_sample):\n",
    "                yield current_sample\n",
    "            current_sample = {\"__key__\": prefix,\n",
    "                              \"__url__\": filesample[\"__url__\"]}\n",
    "        if suffixes is None or suffix in suffixes:\n",
    "            current_sample[suffix] = value\n",
    "    if valid_sample(current_sample):\n",
    "        yield current_sample\n",
    "\n",
    "\n",
    "def tarfile_to_samples_nothrow(src, handler=wds.warn_and_continue):\n",
    "    # NOTE this is a re-impl of the webdataset impl with group_by_keys that doesn't throw\n",
    "    streams = url_opener(src, handler=handler)\n",
    "    files = tar_file_expander(streams, handler=handler)\n",
    "    samples = group_by_keys_nothrow(files, handler=handler)\n",
    "    return samples\n",
    "\n",
    "\n",
    "def filter_keys(key_set):\n",
    "    def _f(dictionary):\n",
    "        return {k: v for k, v in dictionary.items() if k in key_set}\n",
    "\n",
    "    return _f\n",
    "\n",
    "\n",
    "class WebdatasetFilter:\n",
    "    def __init__(self, min_size=1024, max_pwatermark=0.5):\n",
    "        self.min_size = min_size\n",
    "        self.max_pwatermark = max_pwatermark\n",
    "\n",
    "    def __call__(self, x):\n",
    "        try:\n",
    "            if \"json\" in x:\n",
    "                x_json = json.loads(x[\"json\"])\n",
    "                filter_size = (x_json.get(\"original_width\", 0.0) or 0.0) >= self.min_size and x_json.get(\n",
    "                    \"original_height\", 0\n",
    "                ) >= self.min_size\n",
    "                filter_watermark = (x_json.get(\n",
    "                    \"pwatermark\", 1.0) or 1.0) <= self.max_pwatermark\n",
    "                return filter_size and filter_watermark\n",
    "            else:\n",
    "                return False\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "\n",
    "class Text2ImageDataset:\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_shards_path_or_url: Union[str, List[str]],\n",
    "        num_train_examples: int,\n",
    "        per_gpu_batch_size: int,\n",
    "        global_batch_size: int,\n",
    "        num_workers: int,\n",
    "        resolution: int = 1024,\n",
    "        shuffle_buffer_size: int = 1000,\n",
    "        pin_memory: bool = False,\n",
    "        persistent_workers: bool = False,\n",
    "        use_fix_crop_and_size: bool = False,\n",
    "    ):\n",
    "        if not isinstance(train_shards_path_or_url, str):\n",
    "            train_shards_path_or_url = [\n",
    "                list(braceexpand(urls)) for urls in train_shards_path_or_url]\n",
    "            # flatten list using itertools\n",
    "            train_shards_path_or_url = list(\n",
    "                itertools.chain.from_iterable(train_shards_path_or_url))\n",
    "\n",
    "        def get_orig_size(json, resolution=1024, use_fix_crop_and_size=False):\n",
    "            if use_fix_crop_and_size:\n",
    "                return (resolution, resolution, str(json.get(\"caption\", \"\")))\n",
    "            else:\n",
    "                return (int(json.get(\"original_width\", 0.0)), int(json.get(\"original_height\", 0.0)),\n",
    "                        str(json.get(\"caption\", \"\")))\n",
    "\n",
    "        def transform(example):\n",
    "            # resize image\n",
    "            image = example[\"image\"]\n",
    "            image = TF.resize(\n",
    "                image, resolution, interpolation=transforms.InterpolationMode.BILINEAR)\n",
    "\n",
    "            # get crop coordinates and crop image\n",
    "            c_top, c_left, _, _ = transforms.RandomCrop.get_params(\n",
    "                image, output_size=(resolution, resolution))\n",
    "            image = TF.crop(image, c_top, c_left, resolution, resolution)\n",
    "            image = TF.to_tensor(image)\n",
    "            image = TF.normalize(image, [0.5], [0.5])\n",
    "\n",
    "            example[\"image\"] = image\n",
    "            example[\"crop_coords\"] = (\n",
    "                c_top, c_left) if not use_fix_crop_and_size else (0, 0)\n",
    "            return example\n",
    "\n",
    "        processing_pipeline = [\n",
    "            wds.decode(\"pil\", handler=wds.ignore_and_continue),\n",
    "            wds.rename(image=\"jpg;png;jpeg;webp\", orig_size=\"json\",\n",
    "                       handler=wds.warn_and_continue),\n",
    "            wds.map(filter_keys({\"image\", \"orig_size\"})),\n",
    "            wds.map_dict(orig_size=get_orig_size),\n",
    "            wds.map(transform),\n",
    "            wds.to_tuple(\"image\", \"orig_size\", \"crop_coords\"),\n",
    "        ]\n",
    "\n",
    "        # Create train dataset and loader\n",
    "        pipeline = [\n",
    "            wds.ResampledShards(train_shards_path_or_url),\n",
    "            tarfile_to_samples_nothrow,\n",
    "            # wds.select(WebdatasetFilter(min_size=960)),\n",
    "            wds.shuffle(shuffle_buffer_size),\n",
    "            *processing_pipeline,\n",
    "            wds.batched(per_gpu_batch_size, partial=False,\n",
    "                        collation_fn=default_collate),\n",
    "        ]\n",
    "\n",
    "        num_worker_batches = math.ceil(\n",
    "            num_train_examples / (global_batch_size * num_workers))  # per dataloader worker\n",
    "        num_batches = num_worker_batches * num_workers\n",
    "        num_samples = num_batches * global_batch_size\n",
    "\n",
    "        # each worker is iterating over this\n",
    "        self._train_dataset = wds.DataPipeline(\n",
    "            *pipeline).with_epoch(num_worker_batches)\n",
    "        self._train_dataloader = wds.WebLoader(\n",
    "            self._train_dataset,\n",
    "            batch_size=None,\n",
    "            shuffle=False,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=pin_memory,\n",
    "            persistent_workers=persistent_workers,\n",
    "        )\n",
    "        # add meta-data to dataloader instance for convenience\n",
    "        self._train_dataloader.num_batches = num_batches\n",
    "        self._train_dataloader.num_samples = num_samples\n",
    "\n",
    "    @property\n",
    "    def train_dataset(self):\n",
    "        return self._train_dataset\n",
    "\n",
    "    @property\n",
    "    def train_dataloader(self):\n",
    "        return self._train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_shards_path_or_url = \"/home/dataset/songyun/songyun_small.tar\"\n",
    "max_train_samples = 10\n",
    "train_batch_size = 4\n",
    "dataloader_num_workers = 1\n",
    "resolution = 1024\n",
    "use_fix_crop_and_size = False\n",
    "\n",
    "dataset = Text2ImageDataset(\n",
    "    train_shards_path_or_url=train_shards_path_or_url,\n",
    "    num_train_examples=max_train_samples,\n",
    "    per_gpu_batch_size=train_batch_size,\n",
    "    global_batch_size=train_batch_size,\n",
    "    num_workers=dataloader_num_workers,\n",
    "    resolution=resolution,\n",
    "    shuffle_buffer_size=1000,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    use_fix_crop_and_size=use_fix_crop_and_size,\n",
    ")\n",
    "\n",
    "train_dataloader = dataset.train_dataloader\n",
    "\n",
    "for step, batch in enumerate(train_dataloader):\n",
    "    image, text, orig_size, crop_coords = batch[0], batch[1][2], [\n",
    "        batch[1][0], batch[1][1]], batch[2]\n",
    "    print(image.shape, text, orig_size, crop_coords)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
